#!/bin/bash

# Skrypt do uruchamiania aplikacji lokalnie (bez Dockera dla Python)
# Bazy danych (PostgreSQL i Redis) sÄ… uruchamiane w Docker

set -e

echo "ğŸš€ Uruchamianie Medical Document Classifier w trybie lokalnym..."

# SprawdÅº czy .env.local istnieje
if [ ! -f .env.local ]; then
    echo "âŒ Brak pliku .env.local! KopiujÄ™ z .env.local.example..."
    cp .env.local .env.local
fi

# Kopiuj .env.local do .env (aplikacja czyta z .env)
cp .env.local .env

echo "âœ… Konfiguracja zaÅ‚adowana z .env.local"

# Baza danych nie jest potrzebna - aplikacja tylko klasyfikuje dokumenty

# SprawdÅº czy lokalne Ollama dziaÅ‚a
if ! pgrep -f "ollama serve" > /dev/null; then
    echo "âš ï¸  Ollama nie jest uruchomione lokalnie!"
    echo "ğŸ’¡ Uruchom Ollama: ollama serve"
    echo "ğŸ’¡ Lub zainstaluj: brew install ollama"
else
    echo "âœ… Ollama dziaÅ‚a lokalnie (z GPU/Metal)"
fi

# SprawdÅº czy virtual environment istnieje
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Tworzenie virtual environment..."
    python3.11 -m venv venv
    echo "âœ… Virtual environment utworzony"
fi

# Aktywuj virtual environment
echo "ğŸ”§ Aktywowanie virtual environment..."
source venv/bin/activate

# Zainstaluj zaleÅ¼noÅ›ci
echo "ğŸ“¥ Instalowanie zaleÅ¼noÅ›ci..."
pip install --upgrade pip
pip install -r requirements.txt

echo ""
echo "âœ… Wszystko gotowe!"
echo ""
echo "ğŸŒ Uruchamianie aplikacji na http://localhost:8000"
echo "ğŸ“š Dokumentacja API: http://localhost:8000/docs"
echo "ğŸ¤– Ollama (LLM, lokalne z GPU): http://localhost:11434"
echo ""
echo "ğŸ’¡ Aby zatrzymaÄ‡ aplikacjÄ™: Ctrl+C"
echo "ğŸ’¡ Lista modeli Ollama: ollama list"
echo ""
echo "â„¹ï¸  Bazy danych wyÅ‚Ä…czone - aplikacja tylko klasyfikuje"
echo ""

# Uruchom aplikacjÄ™
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
